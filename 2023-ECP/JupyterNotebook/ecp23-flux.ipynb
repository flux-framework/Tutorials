{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2507d149-dcab-458a-a554-37388e0ee13a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<center><img src=\"Flux-logo.svg\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e867ba-f689-4301-bb60-9a448556bb84",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Flux RADIUSS Tutorial on AWS\n",
    "\n",
    "### Flux is a flexible framework for resource management, built for your site. The framework consists of a suite of projects, tools, and libraries which may be used to build site-custom resource managers for High Performance Computing centers. Flux is a next-generation resource manager and scheduler with many transformative capabilities like hierarchical scheduling and resource management (you can think of it as \"fractal scheduling\") and directed-graph based resource representations.\n",
    "\n",
    "#### To step through examples in this notebook you need to execute cells. To run a cell, press Shift+Enter on your keyboard. If you prefer, you can also paste the shell commands in the JupyterLab terminal and execute them there.\n",
    "\n",
    "#### Let's get started! To provide some brief, added background on Flux and a bit more motivation for our tutorial, \"Shift+Enter\" the cell below to watch our YouTube video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ecd22-8552-4b4d-9bc4-61d86f8d33fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"640\" height=\"360\" \n",
    "    src=\"https://www.youtube.com/embed/YIwt51dyXOE\" \n",
    "    title=\"YouTube video player\" \n",
    "    frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" \n",
    "    allowfullscreen>S\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e82c38-8465-49ac-ae2b-b0bb56a79ec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting started with Flux\n",
    "\n",
    "### The code and examples that this tutorial is based on can be found at https://github.com/flux-framework/flux-workflow-examples.git. You can also find the examples in the flux-workflow-examples directory in this JupyterLab instance.\n",
    "\n",
    "### Navigate to https://flux-framework.org/ for the full Flux-Framework, including [documentation](https://flux-framework.readthedocs.io/en/latest/).\n",
    "\n",
    "To read the Flux manpages and get help, run `flux help`. To get documentation on a subcommand, run, e.g. `flux help config`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d616de-70cd-4090-bd43-ffacb5ade1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!flux help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33fef6-278c-4996-8534-fd15e548b338",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get documentation on a subcommand, run, e.g. `flux help jobs`. Note the unique hierarchical features of Flux in the output via recursive inspection:\n",
    "\n",
    "       -R, --recursive\n",
    "              List  jobs recursively. Each child job which is also an instance\n",
    "              of Flux is prefixed by its jobid \"path\" followed by the list  of\n",
    "              jobs,  recursively up to any defined -L, --level. If the --stats\n",
    "              option is used, then each child instance  in  the  hierarchy  is\n",
    "              listed with its stats.\n",
    "\n",
    "       --recurse-all\n",
    "              By  default,  jobs  not  owned by the user running flux jobs are\n",
    "              skipped with -R, --recursive, because  normally  Flux  instances\n",
    "              only  permit  the  instance owner to connect. This option forces\n",
    "              the command to attempt to recurse into the jobs of other  users.\n",
    "              Implies --recursive.\n",
    "\n",
    "       -L, --level=N\n",
    "              With  -R,  --recursive,  stop  recursive job listing at level N.\n",
    "              Levels are counted starting at 0, so flux jobs -R  --level=0  is\n",
    "              equivalent  to  flux  jobs without -R, and --level=1 would limit\n",
    "              recursive job listing to child jobs of the current instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54f640-283a-4523-8dde-9617fd6ef0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!flux help jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e435d6-0927-4966-a4d7-47a128c94158",
   "metadata": {
    "tags": []
   },
   "source": [
    "### You can run any of the commands and examples that follow in the JupyterLab terminal. You can find the terminal in the JupyterLab launcher.\n",
    "You'll see a prompt like this: \n",
    "\n",
    "`ƒ(s=4,d=0) fluxuser@6e0f43fd90eb:~$`\n",
    "\n",
    "`s=4` indicates the number of running Flux brokers, `d=0` indicates the Flux hierarchy depth. `@6e0f43fd90eb` references the host, which is a Docker container for our tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3df1d-32c9-4996-b6f7-2fa85f4c02ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating Flux Instances\n",
    "\n",
    "A Flux instance is a fully functional set of services which manage compute resources under its domain with the capability to launch jobs on those resources. A Flux instance may be running as the default resource manager on a cluster, a job in a resource manager such as Slurm, LSF, or Flux itself, or as a test instance launched locally.\n",
    "\n",
    "When run as a job in another resource manager, Flux is started like an MPI program, e.g., `srun [OPTIONS] flux start [SCRIPT]`. Flux is unique in that a test instance which mimics a multi-node instance can be started locally with simply `flux start --test-size=N`. This offers users to a way to learn and test interfaces and commands without access to an HPC cluster.\n",
    "\n",
    "To start a Flux session with 4 brokers in your container, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568de50-f9e0-452f-8364-e52853013d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux start --test-size=4 flux getattr size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693f2d9-651f-4f58-bf53-62528caa83d9",
   "metadata": {},
   "source": [
    "The output indicates the number of brokers started successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1a33c-9f9e-4ba0-a013-e97601f79e41",
   "metadata": {},
   "source": [
    "## Flux uptime\n",
    "Flux provides an `uptime` utility to display properties of the Flux instance such as state of the current instance, how long it has been running, its size and if scheduling is disabled or stopped. The output shows how long the instance has been up, the instance owner, the instance depth (depth in the Flux hierarchy), and the size of the instance (number of brokers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057ce25-d1b3-4cc6-b26a-4b05a1639616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!flux uptime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2d6af-43fa-490e-88e9-10f13e660125",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Submitting Jobs to Flux\n",
    "## Submission CLI\n",
    "### `flux mini`: the Minimal Job Submission Tool\n",
    "\n",
    "To submit jobs to Flux, you can use the `flux mini` command, which has several sub-commands: `submit`, `run`, `bulksubmit`, `batch`, and `alloc`.  The `flux mini submit` command submits a job to Flux and prints out the jobid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e7d41-1d8d-426c-8198-0ad4a57e7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini submit hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4c25e-3ca8-4277-bb70-a0e94bcd223b",
   "metadata": {},
   "source": [
    "`mini submit` supports common options like `--nnodes`, `--ntasks`, and `--cores-per-task`. There are short option equivalents (`-N`, `-n`, and `-c`, respectively) of these options as well. `--cores-per-task=1` is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d8c3d-b24a-415e-b9ac-f58b99a7e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini submit -N1 -n2 sleep inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bddee-f454-4674-80d4-4a39c5f1bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini submit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac798095",
   "metadata": {},
   "source": [
    "The `flux mini run` command submits a job to Flux (similar to `flux mini submit`) but then attaches to the job with `flux job attach`, printing the job's stdout/stderr to the terminal and exiting with the same exit code as the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d26496-dd1f-44f7-bb10-8a9b4b8c9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini run hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53357a9d-11d8-4c2d-87d8-c30ae38d01ba",
   "metadata": {},
   "source": [
    "The output from the previous command is the hostname (a container ID string in this case). If the job exits with a non-zero exit code this will be reported by `flux job attach` (occurs implicitly with `flux mini run`). For example, execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40cb98-a138-4771-a7ef-f1860dddf7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini run /bin/false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b5c3f-e24a-45a8-a10c-e10bfdbb7b87",
   "metadata": {},
   "source": [
    "A job submitted with `mini run` can be canceled with two rapid `Cltr-C`s in succession, or a user can detach from the job with `Ctrl-C Ctrl-Z`. The user can then re-attach to the job by using `flux job attach JOBID`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5213d",
   "metadata": {},
   "source": [
    "`flux mini submit` and `flux mini run` also support many other useful flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02032748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini run -n4 --label-io --time-limit=5s --env-remove=LD_LIBRARY_PATH hostname\n",
    "!flux mini run --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e9ed6c",
   "metadata": {},
   "source": [
    "The `flux mini bulksubmit` command enqueues jobs based on a set of inputs which are substituted on the command line, similar to `xargs` and the GNU `parallel` utility, except the jobs have access to the resources of an entire Flux instance instead of only the local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e82702",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini bulksubmit --watch --wait echo {} ::: foo bar baz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392a8056-1661-4b76-9ca3-5e536c687e82",
   "metadata": {},
   "source": [
    "The `--cc` option to `mini submit` makes repeated submission even easier via, `flux mini submit --cc=IDSET`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1962b-1831-4bd2-8dab-c61fd710df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini submit --cc=1-10 --watch hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca3706-8bb4-4fd6-a37c-e6135fb05604",
   "metadata": {},
   "source": [
    "Try it in the JupyterLab terminal with a progress bar and jobs/s rate report: `flux mini submit --cc=1-100 --watch --progress --jps hostname`\n",
    "\n",
    "Note that `--wait` is implied by `--watch`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a18ff-8d6a-47e9-a164-931ed1275ef4",
   "metadata": {},
   "source": [
    "Of course, Flux can launch more than just single-node, single-core jobs.  We can submit multiple heterogeneous jobs and Flux will co-schedule the jobs while also ensuring no oversubscription of resources (e.g., cores).\n",
    "\n",
    "Note: in this tutorial, we cannot assume that the host you are running on has multiple cores, thus the examples below only vary the number of nodes per job.  Varying the `cores-per-task` is also possible on Flux when the underlying hardware supports it (e.g., a multi-core node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini submit --nodes=2 --ntasks=2 --cores-per-task=1 --job-name simulation sleep inf\n",
    "!flux mini submit --nodes=1 --ntasks=1 --cores-per-task=1 --job-name analysis sleep inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c2af2",
   "metadata": {},
   "source": [
    "### Listing job properties with `flux jobs`\n",
    "\n",
    "We can now list the jobs in the queue with `flux jobs` and we should see both jobs that we just submitted. Jobs that are instances are colored blue in output, red jobs are failed jobs, and green jobs are those that completed successfully. Note that the JupyterLab notebook may not display these colors. You will be able to see them in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca4277",
   "metadata": {},
   "source": [
    "Since those jobs won't ever exit (and we didn't specify a timelimit), let's kill them off now and free up the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd8ec8-6c64-4d8d-9a00-949f5f58c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux job killall -f\n",
    "!flux jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544aa0a9",
   "metadata": {},
   "source": [
    "We can use the `flux mini batch` command to easily created nested flux instances.  When `flux mini batch` is invoked, Flux will automatically create a nested instance that spans the resources allocated to the job, and then Flux runs the batch script passed to `flux mini batch` on rank 0 of the nested instance. \"Rank\" refers to the rank of the Tree-Based Overlay Network (TBON) used by the Flux brokers: https://flux-framework.readthedocs.io/projects/flux-core/en/latest/man1/flux-broker.html\n",
    "\n",
    "While a batch script is expected to launch parallel jobs using `flux mini run` or `flux mini submit` at this level, nothing prevents the script from further batching other sub-batch-jobs using the `flux mini batch` interface, if desired.\n",
    "\n",
    "Note: Flux also provides a `flux mini alloc` which is an interactive version of `flux mini batch`, but demonstrating that in a Jupyter notebook is difficult due to the lack of pseudo-terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini batch --nslots=2 --cores-per-slot=1 --nodes=2 ./sleep_batch.sh\n",
    "!flux mini batch --nslots=2 --cores-per-slot=1 --nodes=2 ./sleep_batch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98bfa1",
   "metadata": {},
   "source": [
    "The contents of `sleep_batch.sh`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-witch",
   "metadata": {},
   "source": [
    "``` bash \n",
    "    #!/bin/bash\n",
    "  \n",
    "    echo \"Starting my batch job\"\n",
    "    echo \"Print the resources allocated to this batch job\"\n",
    "    flux resource list\n",
    "\n",
    "    echo \"Use sleep to emulate a parallel program\"\n",
    "    echo \"Run the program at a total of 2 processes each requiring\"\n",
    "    echo \"1 core. These processes are equally spread across 2 nodes.\"\n",
    "    flux mini run -N 2 -n 2 sleep 30\n",
    "    flux mini run -N 2 -n 2 sleep 30\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff8993-3c39-4f46-939d-4c8be5739fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux jobs\n",
    "\n",
    "# Copy the JOBID from one of the `flux mini batch`s above and paste it in the line below to examine the job's resources and output\n",
    "JOBID=\"ƒFoRYVpt7\"\n",
    "!flux job info {JOBID} R\n",
    "!flux job attach {JOBID}\n",
    "!cat flux-{JOBID}.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e525e2-6c89-4c14-9fae-d87a0d4fc574",
   "metadata": {},
   "source": [
    "To list all completed jobs, run `flux jobs -a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a8b7c-f475-4a51-8bc6-9983dc9d78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux jobs -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e415ecc-f451-4909-a2bf-351a639cd7fa",
   "metadata": {},
   "source": [
    "To restrict the output to failed (i.e., jobs that exit with nonzero exit code, time out, or are canceled or killed) jobs, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032597d2-4b02-47ea-a5e5-915313cdd7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux jobs -f failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2ae62-3e3b-4c82-a0c7-4c97ff1376d2",
   "metadata": {},
   "source": [
    "# Flux Process and Job Utilities\n",
    "## Flux top\n",
    "Flux provides a feature-full version of `top` for nested Flux instances and jobs. In the JupyterLab terminal, invoke `flux top` to see the \"sleep\" jobs. If they have already completed you can resubmit them. \n",
    "\n",
    "We recommend not running `flux top` in the notebook as it is not designed to display output from a command that runs continuously.\n",
    "\n",
    "## Flux pstree\n",
    "In analogy to `top`, Flux provides `flux pstree`. Try it out in the JupyterLab terminal or here in the notebook.\n",
    "\n",
    "## Flux proxy\n",
    "### Interacting with a job hierarchy with `flux proxy`\n",
    "#### Example 1\n",
    "Routes messages to and from a Flux instance. We can use `flux proxy` to connect to a running Flux instance and then submit more nested jobs inside it. You may want to edit `sleep_batch.sh` with the JupyterLab text editor (double click the file in the window on the left) to sleep for `60` or `120` seconds. Then from the JupyterLab terminal, run: \n",
    "\n",
    "`flux mini batch --nslots=2 --cores-per-slot=1 --nodes=2 ./sleep_batch.sh` # outputs JOBID\n",
    "\n",
    "`flux pstree -x`\n",
    "\n",
    "`flux proxy JOBID` # this connects you to the Flux instance corresponding to JOBID above\n",
    "\n",
    "`flux uptime` # Note the depth is now 1 and the size is 2: we're one level deeper in a Flux hierarchy and we have only 2 brokers now.\n",
    "\n",
    "`flux resource list` # This instance has 2 \"nodes\" and 2 cores allocated to it\n",
    "\n",
    "`flux top`\n",
    "\n",
    "#### Example 2\n",
    "Contents of `sleeps.sh`:\n",
    "\n",
    "``` bash \n",
    "#!/bin/bash\n",
    "\n",
    "flux mini submit -N1 sleep 30\n",
    "flux mini submit -N1 sleep 30\n",
    "flux mini submit -N1 sleep 30\n",
    "flux mini submit -N1 sleep 30\n",
    "flux queue drain\n",
    "```\n",
    "Note `flux queue drain` which waits for the sub-jobs to complete.\n",
    "`flux mini batch -N2 sleeps.sh`\n",
    "(copy output JOBID)\n",
    "`flux proxy JOBID`\n",
    "`flux jobs`\n",
    "#### Example 3\n",
    "Here's an example of submitting jubs within a nested instance. You can run this example here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6976f8-dbb6-405e-a06b-47c571aa1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat sub_job1.sh\n",
    "!cat sub_job2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640a611-38e4-42b1-a913-89e0c76c8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux mini batch -N1 ./sub_job1.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b1f0b-e6c2-4583-8068-7c76fa341884",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux pstree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997faffc",
   "metadata": {},
   "source": [
    "## Submission API\n",
    "Flux also provides first-class python bindings which can be used to submit jobs programmatically. The following script shows this with the `flux.job.submit()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import flux\n",
    "from flux.job import JobspecV1\n",
    "from flux.job.JobID import JobID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = flux.Flux() # connect to the running Flux instance\n",
    "compute_jobreq = JobspecV1.from_command(\n",
    "    command=[\"./compute.py\", \"120\"], num_tasks=1, num_nodes=1, cores_per_task=1\n",
    ") # construct a jobspec\n",
    "compute_jobreq.cwd = os.path.expanduser(\"~/flux-workflow-examples/job-submit-api/\") # set the CWD\n",
    "print(JobID(flux.job.submit(f,compute_jobreq)).f58) # submit and print out the jobid (in f58 format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d679897-7054-4f96-b340-7f39245aca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332f9c9",
   "metadata": {},
   "source": [
    "Under the hood, the `Jobspec` class is creating a YAML document that ultimately gets serialized as JSON and sent to Flux for ingestion, validation, queueing, scheduling, and eventually execution.  We can dump the raw JSON jobspec that is submitted, where we can see the exact resources requested and the task set to be executed on those resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa06478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_jobreq.dumps(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbc90e",
   "metadata": {},
   "source": [
    "We can then replicate our previous example of submitting multiple heterogeneous jobs and testing that Flux co-schedules them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_jobreq = JobspecV1.from_command(\n",
    "    command=[\"./compute.py\", \"120\"], num_tasks=4, num_nodes=2, cores_per_task=2\n",
    ")\n",
    "compute_jobreq.cwd = os.path.expanduser(\"~/flux-workflow-examples/job-submit-api/\")\n",
    "print(JobID(flux.job.submit(f, compute_jobreq)))\n",
    "\n",
    "io_jobreq = JobspecV1.from_command(\n",
    "    command=[\"./io-forwarding.py\", \"120\"], num_tasks=1, num_nodes=1, cores_per_task=1\n",
    ")\n",
    "io_jobreq.cwd = os.path.expanduser(\"~/flux-workflow-examples/job-submit-api/\")\n",
    "print(JobID(flux.job.submit(f, io_jobreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8051640",
   "metadata": {},
   "source": [
    "We can use the FluxExecutor class to submit large numbers of jobs to Flux. This method uses python's `concurrent.futures` interface.  Example snippet from `~/flux-workflow-examples/async-bulk-job-submit/bulksubmit_executor.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-trace",
   "metadata": {},
   "source": [
    "``` python \n",
    "with FluxExecutor() as executor:\n",
    "        compute_jobspec = JobspecV1.from_command(args.command)\n",
    "        futures = [executor.submit(compute_jobspec) for _ in range(args.njobs)]\n",
    "        # wait for the jobid for each job, as a proxy for the job being submitted\n",
    "        for fut in futures:\n",
    "            fut.jobid()\n",
    "        # all jobs submitted - print timings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit a FluxExecutor based script.\n",
    "%run ./flux-workflow-examples/async-bulk-job-submit/bulksubmit_executor.py -n200 /bin/sleep 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec052119",
   "metadata": {},
   "source": [
    "# Diving Deeper Into Flux's Internals\n",
    "\n",
    "Flux uses [hwloc](https://github.com/open-mpi/hwloc) to detect the resources on each node and then to populate its resource graph.\n",
    "\n",
    "You can access the topology information that Flux collects with the `flux resource` subcommand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux resource list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086e47e",
   "metadata": {},
   "source": [
    "Flux can also bootstrap its resource graph based on static input files, like in the case of a multi-user system instance setup by site adminstrators.  [More information on Flux's static resource configuration files](https://flux-framework.readthedocs.io/en/latest/adminguide.html#resource-configuration).  Flux provides a more standard interface to listing available resources that works regardless of the resource input source: `flux resource`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view status of resources\n",
    "!flux resource status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1c49d",
   "metadata": {},
   "source": [
    "Flux has a command for controlling the queue within the `job-manager`: `flux queue`.  This includes disabling job submission, re-enabling it, waiting for the queue to become idle or empty, and checking the queue status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800de4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux queue disable \"maintenance outage\"\n",
    "!flux queue enable\n",
    "!flux queue -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa7559",
   "metadata": {},
   "source": [
    "Each Flux instance has a set of attributes that are set at startup that affect the operation of Flux, such as `rank`, `size`, and `local-uri` (the Unix socket usable for communicating with Flux).  Many of these attributes can be modified at runtime, such as `log-stderr-level` (1 logs only critical messages to stderr while 7 logs everything, including debug messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux getattr rank\n",
    "!flux getattr size\n",
    "!flux getattr local-uri\n",
    "!flux setattr log-stderr-level 3\n",
    "!flux lsattr -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fdfcf",
   "metadata": {},
   "source": [
    "Services within a Flux instance are implemented by modules. To query and manage broker modules, use `flux module`.  Modules that we have already directly interacted with in this tutorial include `resource` (via `flux resource`), `job-ingest` (via `flux mini` and the Python API) `job-list` (via `flux jobs`) and `job-manager` (via `flux queue`), and we will interact with the `kvs` module in a few cells. For the most part, services are implemented by modules of the same name (e.g., `kvs` implements the `kvs` service and thus the `kvs.lookup` RPC).  In some circumstances, where multiple implementations for a service exist, a module of a different name implements a given service (e.g., in this instance, `sched-fluxion-qmanager` provides the `sched` service and thus `sched.alloc`, but in another instance `sched-simple` might provide the `sched` service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7090eb",
   "metadata": {},
   "source": [
    "We can actually unload the Fluxion modules (the scheduler modules from flux-sched) and replace them with `sched-simple` (the scheduler that comes built-into flux-core) as a demonstration of this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4bc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux module unload sched-fluxion-qmanager\n",
    "!flux module unload sched-fluxion-resource\n",
    "!flux module load sched-simple\n",
    "!flux module list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c4ecf",
   "metadata": {},
   "source": [
    "We can now reload the Fluxion scheduler, but this time, let's pass some extra arguments to specialize our Flux instance.  In particular, let's populate our resource graph with nodes, sockets, and cores and limit the scheduling depth to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34899ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux dmesg -C\n",
    "!flux module unload sched-simple\n",
    "!flux module load sched-fluxion-resource load-allowlist=node,socket,core\n",
    "!flux module load sched-fluxion-qmanager queue-params=queue-depth=4\n",
    "!flux module list\n",
    "!flux dmesg | grep queue-depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4b0e04",
   "metadata": {},
   "source": [
    "The key-value store (KVS) is a core component of a Flux instance. The `flux kvs` command provides a utility to list and manipulate values of the KVS. Modules of Flux use the KVS to persistently store information and retrieve it later on (potentially after a restart of Flux).  One example of KVS use by Flux is the `resource` module, which stores the resource set `R` of the current Flux instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux kvs ls \n",
    "!flux kvs ls resource\n",
    "!flux kvs get resource.R | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3920f9e",
   "metadata": {},
   "source": [
    "Flux provides a built-in mechanism for executing commands on nodes without requiring a job or resource allocation: `flux exec`.  `flux exec` is typically used by sys admins to execute administrative commands and load/unload modules across multiple ranks simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9507c7b-de5c-4129-9a99-c943614a9ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 2 flux getattr rank # only execute on rank 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9de119-abc4-4917-a339-2010ccc7b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec flux getattr rank # execute on all ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e716ec-cd20-4330-875b-307d8632f8e4",
   "metadata": {},
   "source": [
    "# DYAD\n",
    "\n",
    "Description/Intro to DYAD Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b022a1e-09c4-4b3b-8248-2e1d7af46a09",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "To demonstrate DYAD's capabilities, we will run the simple demo found in `./dyad_demo/`. This demo consists of a single producer and a single consumer. The producer program generates several files, each consisting of 10 32-bit integers, and registers them with DYAD. The consumer program uses DYAD to retrieve the generated files from the producer's Flux broker. Then, the consumer program reads and validates the contents of the files.\n",
    "\n",
    "There are two implementations of the producer and consumer programs: one in C and one in C++. These two implementations illustrate the use of DYAD's two APIs (i.e., the preload-based C API and the file stream-based C++ API).\n",
    "\n",
    "The cells below show how to manually setup DYAD and run the producer and consumer applications. Note that several of these cells contain `flux exec -r 0,1` commands. These commands are present because we are only going to run on Flux brokers 0 and 1 in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930110ad-0144-4023-aec8-67a220427063",
   "metadata": {},
   "source": [
    "### 1. Make sure the producer and consumer managed directories exist and are empty\n",
    "\n",
    "Before starting DYAD, we need to make sure the directories that DYAD will track for the producer and consumer exist. To start this step, we set `consumer_managed_directory` to the absolute path to the directory that the consumer will manage, and we set `producer_managed_directory` to the absolute path to the directory that the producer will manage. Then, we create those directories, deleting them first if they already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44126340-83e9-491d-8f68-3bc9744c6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_managed_directory = \"/tmp/cons\"\n",
    "producer_managed_directory = \"/tmp/prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e96c0-dde0-4485-b2b5-6b5b677e0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 0,1 rm -rf {consumer_managed_directory}\n",
    "!flux exec -r 0,1 mkdir -p {consumer_managed_directory}\n",
    "!flux exec -r 0,1 chmod 755 {consumer_managed_directory}\n",
    "!flux exec -r 0,1 rm -rf {producer_managed_directory}\n",
    "!flux exec -r 0,1 mkdir -p {producer_managed_directory}\n",
    "!flux exec -r 0,1 chmod 755 {producer_managed_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca3d220-95d6-4dac-8ff3-58f05a8747fe",
   "metadata": {},
   "source": [
    "### 2. Setup the Flux KVS namespace that will be used by DYAD\n",
    "\n",
    "The next step in launching DYAD is to setup the Flux KVS namespace that DYAD will use to store information about produced files. To start this step, we set `kvs_namespace` below to the name of the namespace you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a43c8b-1234-4dfb-bf78-b374195800fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kvs_namespace = \"dyad_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeffdee-c0bb-457e-b29e-43ad8dc149c9",
   "metadata": {},
   "source": [
    "Next, we check if the namespace already exists. If it does, we remove that namespace using `flux kvs namespace remove`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26f2b8-a52a-42c7-95f6-0d0c359296ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux kvs namespace list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92e261-441e-482d-84d3-cb2df8de7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux kvs namespace remove {kvs_namespace}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf32970-faba-4648-9ba1-cd8db7c51e67",
   "metadata": {},
   "source": [
    "Finally, we create the desired namespace using `flux kvs namespace create`. After running this command, the desired namespace should appear in the output of `flux kvs namespace list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5844f-ccf3-44f4-9896-b17874a5fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux kvs namespace create {kvs_namespace}\n",
    "!flux kvs namespace list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c0dda-8a39-460d-9db0-8b936e2e0582",
   "metadata": {},
   "source": [
    "### 3. Startup the DYAD module on the desired Flux brokers\n",
    "\n",
    "The next step in using DYAD is to load DYAD's Flux module. This module is the component of DYAD that actually sends files from producer to consumer. To start this step, we set `dyad_module` below to the path to `dyad.so`. For this demo, DYAD has already been installed under the `/usr` prefix, so the path should be `/usr/lib/dyad.so`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c938e-1cc2-4526-a649-99e4785c687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyad_module = \"/usr/lib/dyad.so\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1604fa8-2c24-4813-adc3-3b141fe9c229",
   "metadata": {},
   "source": [
    "Next, we check if the module has already been loaded by running `flux module list`. If no `dyad` entries are output, then the module is not loaded. If the module is loaded, we unload it using `flux module remove dyad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea5678-7f7c-43d7-8cec-055d938f3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 0,1 flux module list | grep dyad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032bbe2-30ab-405c-a821-1a590314f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 0,1 flux module remove dyad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da9ddd-88e4-45a1-a323-d6f6756d15ab",
   "metadata": {},
   "source": [
    "Finally, we load the DYAD module using `flux module load`. When being loaded, the DYAD module takes a single command-line argument: the producer's managed directory. After loading the module, we double check that it has been loaded by running `flux module list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a54a5-206c-43c1-a7ce-aa0737b698a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 0,1 flux module load {dyad_module} {producer_managed_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e455b60-39e4-4f0c-bee2-77a74759ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 0,1 flux module list | grep dyad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d834ae7-2514-46e6-a169-e9919e037e9e",
   "metadata": {},
   "source": [
    "### 4. Launch the Producer and Consumer Programs\n",
    "\n",
    "Now, we will launch the producer and consumer applications. To start, we need to specify the specific producer and consumer programs we wish to run. These programs are pre-built and can be found in `./dyad_demo/`. For both producer and consumer, there is a C and a C++ version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0efba6f9-5f8f-4f28-9730-4a6243ca0adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_program = \"./dyad_demo/c_prod\" # Change to \"./dyad_demo/cpp_prod\" for C++\n",
    "consumer_program = \"./dyad_demo/c_cons\" # Change to \"./dyad_demo/cpp_cons\" for C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce515836-c2f5-41dd-9bcf-63bdbffc0b58",
   "metadata": {},
   "source": [
    "Next, we need to specify the number of files we wish to generate and transfer by setting the `num_files_transfered` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b050eda-0a42-417f-b282-5611feba22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files_transfered = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0bd36-9edb-4f43-8943-61b7f05851bc",
   "metadata": {},
   "source": [
    "Next, we generate the shell commands to run the producer and consumer. These commands use `flux exec` to ensure the producer and consumer run on the correct Flux brokers. These commands also make use of the `dyadrun` script to configure the DYAD API. This configuration can also be done using environment variables. The flags passed to `dyadrun` in our generated commands do the following:\n",
    "* `--preload`: preloads `/usr/lib/libdyad_sync.so` to intercept `open`, `close`, `fopen`, and `fclose`. This flag is only provided below if the C versions of the producer or consumer are selected\n",
    "* `--kvs_namespace <NAMESPACE>`: specifies which Flux KVS namespace should be used to store information about produced files\n",
    "* `-path <MANAGED_PATH>`: specifies the producer or consumer path that DYAD is managing\n",
    "* `--producer` and `--consumer`: specifies whether the application is a producer or a consumer. Currently, only one of these can be specified. This also affects the `-path` flag. If using `--producer`, the value for `-path` should be the producer path. If using `--consumer`, the value for `-path` should be the consumer path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f711d9-c4cc-41e0-86ea-9f3ffc79fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_launch_cmd = \"flux exec -r 0 \\\n",
    "dyadrun {preload} --producer --kvs_namespace {kvs_namespace} -path {producer_managed_directory} \\\n",
    "{producer_program} {num_files_transfered} {producer_managed_directory}\".format(\n",
    "    preload=\"--preload\" if producer_program.split(\"/\")[-1].strip().startswith(\"c_\") else \"\",\n",
    "    kvs_namespace=kvs_namespace,\n",
    "    producer_managed_directory=producer_managed_directory,\n",
    "    producer_program=producer_program,\n",
    "    num_files_transfered=num_files_transfered,\n",
    ")\n",
    "print(producer_launch_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b70abb-9bd0-490a-98c0-29c7d6fb5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_launch_cmd = \"flux exec -r 1 \\\n",
    "dyadrun {preload} --consumer --kvs_namespace {kvs_namespace} -path {consumer_managed_directory} \\\n",
    "{consumer_program} {num_files_transfered} {consumer_managed_directory}\".format(\n",
    "    preload=\"--preload\" if consumer_program.split(\"/\")[-1].strip().startswith(\"c_\") else \"\",\n",
    "    kvs_namespace=kvs_namespace,\n",
    "    consumer_managed_directory=consumer_managed_directory,\n",
    "    consumer_program=consumer_program,\n",
    "    num_files_transfered=num_files_transfered,\n",
    ")\n",
    "print(consumer_launch_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8fe5a-8864-4abd-9f20-181228cbf7c6",
   "metadata": {},
   "source": [
    "Finally, we run the producer and consumer applications. Thanks to DYAD's fine-grained synchronization features, the order in which we launch the applications does not matter. We know that the applications ran successfully if the consumer outputs \"OK\" for each file it checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b95b2-8e8f-4e79-bd98-824779d0d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{producer_launch_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192b6e6-da63-42e7-ab8c-4c964fbc0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{consumer_launch_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d868b-aa3e-449a-baf2-fc452a440f8c",
   "metadata": {},
   "source": [
    "### 5. Check that the files were transfered\n",
    "\n",
    "To see that the files were transfered, we can check the contents of the producer managed directory and the consumer managed directory from the brokers on which the respective applications ran. When we run the cells below, we will see the same files are present in both directories. This indicates that DYAD transfered the files correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b0e83-603e-4d93-a36f-c8c57e0e8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 0 ls -lah {producer_managed_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e78308-c504-4e8f-9ed0-f835e29f04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 1 ls -lah {consumer_managed_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354e99a-9443-49c1-94de-184724bb6ff4",
   "metadata": {},
   "source": [
    "### 6. Cleanup and tear down DYAD\n",
    "\n",
    "To cleanup and tear down DYAD, we simply unload the DYAD module from the brokers and remove the Flux KVS namespace that DYAD used. We can check if this works by running `flux module list` and `flux kvs namespace list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b848013-c8be-47fa-8de7-ca9d9ea5bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux exec -r 0,1 flux module unload dyad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f9ea6-370e-42ee-ba28-f137bf25a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "!flux kvs namespace remove {kvs_namespace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f8a79-8b35-4923-8bdc-e3e9a9897920",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Modules Post-Cleanup\"\n",
    "!echo \"====================\"\n",
    "!flux module list\n",
    "!echo \"\"\n",
    "!echo \"KVS Namespaces Post-Cleanup\"\n",
    "!echo \"===========================\"\n",
    "!flux kvs namespace list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3e767-0459-4218-a8cf-0f98bd32d6bf",
   "metadata": {},
   "source": [
    "# This concludes the notebook tutorial. \n",
    "\n",
    "## Feel free to experiment more with Flux here or in the terminal. You can try more of the examples in the flux-workflow-examples directory in the window to the left. If you're using a shared system like the one on the RADIUSS AWS tutorial please be mindful of other users and don't run compute intensive workloads. If you're running the tutorial in a job on an HPC cluster compute away!\n",
    "\n",
    "## If you're interested in installing Flux on your cluster, take a look at the system instance instructions here: https://flux-framework.readthedocs.io/en/latest/adminguide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ef448-6921-4503-8cad-7466b528f7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
